{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flax in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: jax>=0.4.27 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (0.5.0)\n",
      "Requirement already satisfied: msgpack in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (1.1.0)\n",
      "Requirement already satisfied: optax in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (0.2.4)\n",
      "Requirement already satisfied: orbax-checkpoint in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (0.11.0)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (0.1.71)\n",
      "Requirement already satisfied: rich>=11.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (13.9.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax) (1.24.4)\n",
      "Requirement already satisfied: jaxlib<=0.5.0,>=0.5.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax>=0.4.27->flax) (0.5.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax>=0.4.27->flax) (0.5.1)\n",
      "Collecting numpy>=1.23.2 (from flax)\n",
      "  Using cached numpy-2.2.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax>=0.4.27->flax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax>=0.4.27->flax) (1.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from rich>=11.1->flax) (2.19.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from optax->flax) (2.1.0)\n",
      "Requirement already satisfied: chex>=0.1.87 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from optax->flax) (0.1.88)\n",
      "Requirement already satisfied: etils[epy] in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from optax->flax) (1.11.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (5.29.3)\n",
      "Requirement already satisfied: humanize in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (4.11.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (3.19.3)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from chex>=0.1.87->optax->flax) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.12.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.21.0)\n",
      "Using cached numpy-2.2.2-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed numpy-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Alexa\\OneDrive\\Bureau\\MD\\MD_tdlog\\.venv\\Lib\\site-packages\\~~mpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: e3x in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (2.1.0)\n",
      "Requirement already satisfied: etils[epath] in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (1.11.0)\n",
      "Requirement already satisfied: flax in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (0.10.2)\n",
      "Requirement already satisfied: jax in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (0.5.0)\n",
      "Requirement already satisfied: jaxtyping in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (0.2.36)\n",
      "Requirement already satisfied: more_itertools in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (10.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (2.2.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from e3x) (1.13.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath]->e3x) (2024.12.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath]->e3x) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath]->e3x) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from etils[epath]->e3x) (3.21.0)\n",
      "Requirement already satisfied: msgpack in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax->e3x) (1.1.0)\n",
      "Requirement already satisfied: optax in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax->e3x) (0.2.4)\n",
      "Requirement already satisfied: orbax-checkpoint in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax->e3x) (0.11.0)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax->e3x) (0.1.71)\n",
      "Requirement already satisfied: rich>=11.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax->e3x) (13.9.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from flax->e3x) (6.0.2)\n",
      "Requirement already satisfied: jaxlib<=0.5.0,>=0.5.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax->e3x) (0.5.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax->e3x) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax->e3x) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from jax->e3x) (1.15.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from sympy->e3x) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from rich>=11.1->flax->e3x) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from rich>=11.1->flax->e3x) (2.19.1)\n",
      "Requirement already satisfied: chex>=0.1.87 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from optax->flax->e3x) (0.1.88)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax->e3x) (1.6.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax->e3x) (5.29.3)\n",
      "Requirement already satisfied: humanize in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax->e3x) (4.11.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from orbax-checkpoint->flax->e3x) (3.19.3)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from chex>=0.1.87->optax->flax->e3x) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alexa\\onedrive\\bureau\\md\\md_tdlog\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->e3x) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install e3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import urllib.request\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import e3x\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset.\n",
    "filename = \"C:/Users/Alexa/OneDrive/Bureau/MD/MD_tdlog/train_ethanol/md17_ethanol.npz\"\n",
    "if not os.path.exists(filename):\n",
    "  print(f\"Downloading {filename} (this may take a while)...\")\n",
    "  urllib.request.urlretrieve(f\"http://www.quantum-machine.org/gdml/data/npz/{filename}\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading md17_ethanol.npz (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('md17_ethanol.npz', <http.client.HTTPMessage at 0x20e1cc3b3d0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset.\n",
    "filename = \"md17_ethanol.npz\"\n",
    "print(f\"Downloading {filename} (this may take a while)...\")\n",
    "urllib.request.urlretrieve(f\"http://www.quantum-machine.org/gdml/data/npz/{filename}\", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"md17_ethanol.npz\", 'r') as zip_ref:\n",
    "    zip_ref.testzip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = np.load(\"md17_ethanol.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(key, num_train, num_valid):\n",
    "  # Load the dataset.\n",
    "  dataset = np.load(filename)\n",
    "\n",
    "  # Make sure that the dataset contains enough entries.\n",
    "  num_data = len(dataset['E'])\n",
    "  num_draw = num_train + num_valid\n",
    "  if num_draw > num_data:\n",
    "    raise RuntimeError(\n",
    "      f'datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}')\n",
    "\n",
    "  # Randomly draw train and validation sets from dataset.\n",
    "  choice = np.asarray(jax.random.choice(key, num_data, shape=(num_draw,), replace=False))\n",
    "  train_choice = choice[:num_train]\n",
    "  valid_choice = choice[num_train:]\n",
    "\n",
    "  # Determine mean energy of the training set.\n",
    "  mean_energy = np.mean(dataset['E'][train_choice])  # ~ -97000\n",
    "\n",
    "  # Collect and return train and validation sets.\n",
    "  train_data = dict(\n",
    "    energy=jnp.asarray(dataset['E'][train_choice, 0] - mean_energy),\n",
    "    forces=jnp.asarray(dataset['F'][train_choice]),\n",
    "    atomic_numbers=jnp.asarray(dataset['z']),\n",
    "    positions=jnp.asarray(dataset['R'][train_choice]),\n",
    "  )\n",
    "  valid_data = dict(\n",
    "    energy=jnp.asarray(dataset['E'][valid_choice, 0] - mean_energy),\n",
    "    forces=jnp.asarray(dataset['F'][valid_choice]),\n",
    "    atomic_numbers=jnp.asarray(dataset['z']),\n",
    "    positions=jnp.asarray(dataset['R'][valid_choice]),\n",
    "  )\n",
    "  return train_data, valid_data, mean_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingModel(nn.Module):\n",
    "  features: int = 32\n",
    "  max_degree: int = 2\n",
    "  num_iterations: int = 3\n",
    "  num_basis_functions: int = 8\n",
    "  cutoff: float = 5.0\n",
    "  max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "\n",
    "  def energy(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size):\n",
    "    # 1. Calculate displacement vectors.\n",
    "    positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "    positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "    displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "    # 2. Expand displacement vectors in basis functions.\n",
    "    basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "      displacements,\n",
    "      num=self.num_basis_functions,\n",
    "      max_degree=self.max_degree,\n",
    "      radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "      cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "    )\n",
    "\n",
    "    # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "    x = e3x.nn.Embed(num_embeddings=self.max_atomic_number+1, features=self.features)(atomic_numbers)\n",
    "\n",
    "    # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "    for i in range(self.num_iterations):\n",
    "      # Message-pass.\n",
    "      if i == self.num_iterations-1:  # Final iteration.\n",
    "        # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "        # features for efficiency reasons.\n",
    "        y = e3x.nn.MessagePass(max_degree=0, include_pseudotensors=False)(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "        # After the final message pass, we can safely throw away all non-scalar features.\n",
    "        x = e3x.nn.change_max_degree_or_type(x, max_degree=0, include_pseudotensors=False)\n",
    "      else:\n",
    "        # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "        y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "      y = e3x.nn.add(x, y)\n",
    "\n",
    "      # Atom-wise refinement MLP.\n",
    "      y = e3x.nn.Dense(self.features)(y)\n",
    "      y = e3x.nn.silu(y)\n",
    "      y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "      # Residual connection.\n",
    "      x = e3x.nn.add(x, y)\n",
    "\n",
    "    # 5. Predict atomic energies with an ordinary dense layer.\n",
    "    element_bias = self.param('element_bias', lambda rng, shape: jnp.zeros(shape), (self.max_atomic_number+1))\n",
    "    atomic_energies = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)  # (..., Natoms, 1, 1, 1)\n",
    "    atomic_energies = jnp.squeeze(atomic_energies, axis=(-1, -2, -3))  # Squeeze last 3 dimensions.\n",
    "    atomic_energies += element_bias[atomic_numbers]\n",
    "\n",
    "    # 6. Sum atomic energies to obtain the total energy.\n",
    "    energy = jax.ops.segment_sum(atomic_energies, segment_ids=batch_segments, num_segments=batch_size)\n",
    "\n",
    "    # To be able to efficiently compute forces, our model should return a single output (instead of one for each\n",
    "    # molecule in the batch). Fortunately, since all atomic contributions only influence the energy in their own\n",
    "    # batch segment, we can simply sum the energy of all molecules in the batch to obtain a single proxy output\n",
    "    # to differentiate.\n",
    "    return -jnp.sum(energy), energy  # Forces are the negative gradient, hence the minus sign.\n",
    "\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments=None, batch_size=None):\n",
    "    if batch_segments is None:\n",
    "      batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "      batch_size = 1\n",
    "\n",
    "    # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "    # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "    energy_and_forces = jax.value_and_grad(self.energy, argnums=1, has_aux=True)\n",
    "    (_, energy), forces = energy_and_forces(atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size)\n",
    "\n",
    "    return energy, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "  # Determine the number of training steps per epoch.\n",
    "  data_size = len(data['energy'])\n",
    "  steps_per_epoch = data_size//batch_size\n",
    "\n",
    "  # Draw random permutations for fetching batches from the train data.\n",
    "  perms = jax.random.permutation(key, data_size)\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  # Prepare entries that are identical for each batch.\n",
    "  num_atoms = len(data['atomic_numbers'])\n",
    "  batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "  atomic_numbers = jnp.tile(data['atomic_numbers'], batch_size)\n",
    "  offsets = jnp.arange(batch_size) * num_atoms\n",
    "  dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "  dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "  src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "  # Assemble and return batches.\n",
    "  return [\n",
    "    dict(\n",
    "        energy=data['energy'][perm],\n",
    "        forces=data['forces'][perm].reshape(-1, 3),\n",
    "        atomic_numbers=atomic_numbers,\n",
    "        positions=data['positions'][perm].reshape(-1, 3),\n",
    "        dst_idx=dst_idx,\n",
    "        src_idx=src_idx,\n",
    "        batch_segments = batch_segments,\n",
    "    )\n",
    "    for perm in perms\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(energy_prediction, energy_target, forces_prediction, forces_target, forces_weight):\n",
    "  energy_loss = jnp.mean(optax.l2_loss(energy_prediction, energy_target))\n",
    "  forces_loss = jnp.mean(optax.l2_loss(forces_prediction, forces_target))\n",
    "  return energy_loss + forces_weight * forces_loss\n",
    "\n",
    "def mean_absolute_error(prediction, target):\n",
    "  return jnp.mean(jnp.abs(prediction - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update', 'batch_size'))\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, forces_weight, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    energy, forces = model_apply(\n",
    "      params,\n",
    "      atomic_numbers=batch['atomic_numbers'],\n",
    "      positions=batch['positions'],\n",
    "      dst_idx=batch['dst_idx'],\n",
    "      src_idx=batch['src_idx'],\n",
    "      batch_segments=batch['batch_segments'],\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "    loss = mean_squared_loss(\n",
    "      energy_prediction=energy,\n",
    "      energy_target=batch['energy'],\n",
    "      forces_prediction=forces,\n",
    "      forces_target=batch['forces'],\n",
    "      forces_weight=forces_weight\n",
    "    )\n",
    "    return loss, (energy, forces)\n",
    "  (loss, (energy, forces)), grad = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  energy_mae = mean_absolute_error(energy, batch['energy'])\n",
    "  forces_mae = mean_absolute_error(forces, batch['forces'])\n",
    "  return params, opt_state, loss, energy_mae, forces_mae\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'batch_size'))\n",
    "def eval_step(model_apply, batch, batch_size, forces_weight, params):\n",
    "  energy, forces = model_apply(\n",
    "    params,\n",
    "    atomic_numbers=batch['atomic_numbers'],\n",
    "    positions=batch['positions'],\n",
    "    dst_idx=batch['dst_idx'],\n",
    "    src_idx=batch['src_idx'],\n",
    "    batch_segments=batch['batch_segments'],\n",
    "    batch_size=batch_size\n",
    "  )\n",
    "  loss = mean_squared_loss(\n",
    "    energy_prediction=energy,\n",
    "    energy_target=batch['energy'],\n",
    "    forces_prediction=forces,\n",
    "    forces_target=batch['forces'],\n",
    "    forces_weight=forces_weight\n",
    "  )\n",
    "  energy_mae = mean_absolute_error(energy, batch['energy'])\n",
    "  forces_mae = mean_absolute_error(forces, batch['forces'])\n",
    "  return loss, energy_mae, forces_mae\n",
    "\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, forces_weight, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(train_data['atomic_numbers']))\n",
    "  params = model.init(init_key,\n",
    "    atomic_numbers=train_data['atomic_numbers'],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Batches for the validation set need to be prepared only once.\n",
    "  key, shuffle_key = jax.random.split(key)\n",
    "  valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Prepare batches.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "    # Loop over train batches.\n",
    "    train_loss = 0.0\n",
    "    train_energy_mae = 0.0\n",
    "    train_forces_mae = 0.0\n",
    "    for i, batch in enumerate(train_batches):\n",
    "      params, opt_state, loss, energy_mae, forces_mae = train_step(\n",
    "        model_apply=model.apply,\n",
    "        optimizer_update=optimizer.update,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        forces_weight=forces_weight,\n",
    "        opt_state=opt_state,\n",
    "        params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "      train_energy_mae += (energy_mae - train_energy_mae)/(i+1)\n",
    "      train_forces_mae += (forces_mae - train_forces_mae)/(i+1)\n",
    "\n",
    "    # Evaluate on validation set.\n",
    "    valid_loss = 0.0\n",
    "    valid_energy_mae = 0.0\n",
    "    valid_forces_mae = 0.0\n",
    "    for i, batch in enumerate(valid_batches):\n",
    "      loss, energy_mae, forces_mae = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        forces_weight=forces_weight,\n",
    "        params=params\n",
    "      )\n",
    "      valid_loss += (loss - valid_loss)/(i+1)\n",
    "      valid_energy_mae += (energy_mae - valid_energy_mae)/(i+1)\n",
    "      valid_forces_mae += (forces_mae - valid_forces_mae)/(i+1)\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "    print(f\"    loss [a.u.]             {train_loss : 8.3f} {valid_loss : 8.3f}\")\n",
    "    print(f\"    energy mae [kcal/mol]   {train_energy_mae: 8.3f} {valid_energy_mae: 8.3f}\")\n",
    "    print(f\"    forces mae [kcal/mol/Å] {train_forces_mae: 8.3f} {valid_forces_mae: 8.3f}\")\n",
    "\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters.\n",
    "features = 32\n",
    "max_degree = 1\n",
    "num_iterations = 3\n",
    "num_basis_functions = 16\n",
    "cutoff = 5.0\n",
    "\n",
    "# Training hyperparameters.\n",
    "num_train = 900\n",
    "num_valid = 100\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "forces_weight = 1.0\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1                    train:   valid:\n",
      "    loss [a.u.]              355.423  356.728\n",
      "    energy mae [kcal/mol]      3.262    2.894\n",
      "    forces mae [kcal/mol/Å]   19.464   19.468\n",
      "epoch:   2                    train:   valid:\n",
      "    loss [a.u.]              354.710  354.423\n",
      "    energy mae [kcal/mol]      3.542    4.487\n",
      "    forces mae [kcal/mol/Å]   19.406   19.253\n",
      "epoch:   3                    train:   valid:\n",
      "    loss [a.u.]              346.704  354.731\n",
      "    energy mae [kcal/mol]      4.641    3.105\n",
      "    forces mae [kcal/mol/Å]   18.939   19.375\n",
      "epoch:   4                    train:   valid:\n",
      "    loss [a.u.]              318.852  196.794\n",
      "    energy mae [kcal/mol]      4.369    3.388\n",
      "    forces mae [kcal/mol/Å]   17.970   14.224\n",
      "epoch:   5                    train:   valid:\n",
      "    loss [a.u.]              139.096  109.011\n",
      "    energy mae [kcal/mol]      3.246    3.240\n",
      "    forces mae [kcal/mol/Å]   11.963   10.686\n",
      "epoch:   6                    train:   valid:\n",
      "    loss [a.u.]               92.328   87.020\n",
      "    energy mae [kcal/mol]      2.935    2.408\n",
      "    forces mae [kcal/mol/Å]    9.799    9.611\n",
      "epoch:   7                    train:   valid:\n",
      "    loss [a.u.]               76.401   67.536\n",
      "    energy mae [kcal/mol]      3.043    2.309\n",
      "    forces mae [kcal/mol/Å]    8.787    8.390\n",
      "epoch:   8                    train:   valid:\n",
      "    loss [a.u.]               62.613   88.977\n",
      "    energy mae [kcal/mol]      3.004    8.285\n",
      "    forces mae [kcal/mol/Å]    7.853    7.587\n",
      "epoch:   9                    train:   valid:\n",
      "    loss [a.u.]               59.709   53.507\n",
      "    energy mae [kcal/mol]      3.126    2.825\n",
      "    forces mae [kcal/mol/Å]    7.541    7.234\n",
      "epoch:  10                    train:   valid:\n",
      "    loss [a.u.]               46.117   71.999\n",
      "    energy mae [kcal/mol]      2.778    7.740\n",
      "    forces mae [kcal/mol/Å]    6.608    6.578\n",
      "epoch:  11                    train:   valid:\n",
      "    loss [a.u.]               49.170   53.807\n",
      "    energy mae [kcal/mol]      3.357    2.825\n",
      "    forces mae [kcal/mol/Å]    6.639    7.306\n",
      "epoch:  12                    train:   valid:\n",
      "    loss [a.u.]               41.466   41.856\n",
      "    energy mae [kcal/mol]      2.711    3.545\n",
      "    forces mae [kcal/mol/Å]    6.255    6.130\n",
      "epoch:  13                    train:   valid:\n",
      "    loss [a.u.]               39.076   40.573\n",
      "    energy mae [kcal/mol]      3.169    1.676\n",
      "    forces mae [kcal/mol/Å]    5.862    6.352\n",
      "epoch:  14                    train:   valid:\n",
      "    loss [a.u.]               31.340   32.961\n",
      "    energy mae [kcal/mol]      2.251    1.768\n",
      "    forces mae [kcal/mol/Å]    5.420    5.761\n",
      "epoch:  15                    train:   valid:\n",
      "    loss [a.u.]               32.456   25.680\n",
      "    energy mae [kcal/mol]      2.642    1.377\n",
      "    forces mae [kcal/mol/Å]    5.368    5.038\n",
      "epoch:  16                    train:   valid:\n",
      "    loss [a.u.]               27.218   29.217\n",
      "    energy mae [kcal/mol]      2.559    1.538\n",
      "    forces mae [kcal/mol/Å]    4.937    5.381\n",
      "epoch:  17                    train:   valid:\n",
      "    loss [a.u.]               26.130   30.467\n",
      "    energy mae [kcal/mol]      2.403    3.511\n",
      "    forces mae [kcal/mol/Å]    4.859    4.981\n",
      "epoch:  18                    train:   valid:\n",
      "    loss [a.u.]               21.764   21.655\n",
      "    energy mae [kcal/mol]      2.314    3.033\n",
      "    forces mae [kcal/mol/Å]    4.397    4.212\n",
      "epoch:  19                    train:   valid:\n",
      "    loss [a.u.]               20.019   20.475\n",
      "    energy mae [kcal/mol]      2.215    2.446\n",
      "    forces mae [kcal/mol/Å]    4.201    4.283\n",
      "epoch:  20                    train:   valid:\n",
      "    loss [a.u.]               19.448   22.526\n",
      "    energy mae [kcal/mol]      2.283    3.104\n",
      "    forces mae [kcal/mol/Å]    4.099    4.305\n",
      "epoch:  21                    train:   valid:\n",
      "    loss [a.u.]               16.305   12.644\n",
      "    energy mae [kcal/mol]      2.042    1.350\n",
      "    forces mae [kcal/mol/Å]    3.761    3.549\n",
      "epoch:  22                    train:   valid:\n",
      "    loss [a.u.]               17.166   17.750\n",
      "    energy mae [kcal/mol]      2.351    2.543\n",
      "    forces mae [kcal/mol/Å]    3.773    3.854\n",
      "epoch:  23                    train:   valid:\n",
      "    loss [a.u.]               16.439   14.009\n",
      "    energy mae [kcal/mol]      2.139    1.322\n",
      "    forces mae [kcal/mol/Å]    3.749    3.678\n",
      "epoch:  24                    train:   valid:\n",
      "    loss [a.u.]               13.537    8.901\n",
      "    energy mae [kcal/mol]      1.778    0.905\n",
      "    forces mae [kcal/mol/Å]    3.473    2.995\n",
      "epoch:  25                    train:   valid:\n",
      "    loss [a.u.]               10.437    8.582\n",
      "    energy mae [kcal/mol]      1.565    0.717\n",
      "    forces mae [kcal/mol/Å]    3.055    3.005\n",
      "epoch:  26                    train:   valid:\n",
      "    loss [a.u.]               10.564    8.294\n",
      "    energy mae [kcal/mol]      1.683    1.801\n",
      "    forces mae [kcal/mol/Å]    3.040    2.657\n",
      "epoch:  27                    train:   valid:\n",
      "    loss [a.u.]               10.285    7.547\n",
      "    energy mae [kcal/mol]      1.764    0.975\n",
      "    forces mae [kcal/mol/Å]    2.931    2.702\n",
      "epoch:  28                    train:   valid:\n",
      "    loss [a.u.]                8.262    7.512\n",
      "    energy mae [kcal/mol]      1.415    1.134\n",
      "    forces mae [kcal/mol/Å]    2.709    2.681\n",
      "epoch:  29                    train:   valid:\n",
      "    loss [a.u.]                7.466   10.519\n",
      "    energy mae [kcal/mol]      1.209    0.867\n",
      "    forces mae [kcal/mol/Å]    2.639    3.348\n",
      "epoch:  30                    train:   valid:\n",
      "    loss [a.u.]                9.588    8.324\n",
      "    energy mae [kcal/mol]      1.778    0.773\n",
      "    forces mae [kcal/mol/Å]    2.767    2.912\n",
      "epoch:  31                    train:   valid:\n",
      "    loss [a.u.]                7.526   10.411\n",
      "    energy mae [kcal/mol]      1.344    0.981\n",
      "    forces mae [kcal/mol/Å]    2.555    3.195\n",
      "epoch:  32                    train:   valid:\n",
      "    loss [a.u.]                9.283    7.451\n",
      "    energy mae [kcal/mol]      1.656    0.860\n",
      "    forces mae [kcal/mol/Å]    2.788    2.748\n",
      "epoch:  33                    train:   valid:\n",
      "    loss [a.u.]                5.774    5.159\n",
      "    energy mae [kcal/mol]      0.995    0.681\n",
      "    forces mae [kcal/mol/Å]    2.319    2.256\n",
      "epoch:  34                    train:   valid:\n",
      "    loss [a.u.]                6.893    5.986\n",
      "    energy mae [kcal/mol]      1.365    0.922\n",
      "    forces mae [kcal/mol/Å]    2.435    2.457\n",
      "epoch:  35                    train:   valid:\n",
      "    loss [a.u.]                6.498    5.422\n",
      "    energy mae [kcal/mol]      1.345    1.143\n",
      "    forces mae [kcal/mol/Å]    2.338    2.284\n",
      "epoch:  36                    train:   valid:\n",
      "    loss [a.u.]                5.862    6.154\n",
      "    energy mae [kcal/mol]      1.206    0.986\n",
      "    forces mae [kcal/mol/Å]    2.257    2.453\n",
      "epoch:  37                    train:   valid:\n",
      "    loss [a.u.]                6.604    7.678\n",
      "    energy mae [kcal/mol]      1.310    0.879\n",
      "    forces mae [kcal/mol/Å]    2.405    2.704\n",
      "epoch:  38                    train:   valid:\n",
      "    loss [a.u.]                6.250    5.954\n",
      "    energy mae [kcal/mol]      1.267    0.821\n",
      "    forces mae [kcal/mol/Å]    2.328    2.404\n",
      "epoch:  39                    train:   valid:\n",
      "    loss [a.u.]                6.894    7.708\n",
      "    energy mae [kcal/mol]      1.256    1.732\n",
      "    forces mae [kcal/mol/Å]    2.468    2.500\n",
      "epoch:  40                    train:   valid:\n",
      "    loss [a.u.]                5.207    5.595\n",
      "    energy mae [kcal/mol]      1.027    1.540\n",
      "    forces mae [kcal/mol/Å]    2.189    2.156\n",
      "epoch:  41                    train:   valid:\n",
      "    loss [a.u.]                5.944    6.216\n",
      "    energy mae [kcal/mol]      1.398    0.879\n",
      "    forces mae [kcal/mol/Å]    2.216    2.494\n",
      "epoch:  42                    train:   valid:\n",
      "    loss [a.u.]                5.317    4.650\n",
      "    energy mae [kcal/mol]      1.088    1.440\n",
      "    forces mae [kcal/mol/Å]    2.195    1.911\n",
      "epoch:  43                    train:   valid:\n",
      "    loss [a.u.]                5.521    4.577\n",
      "    energy mae [kcal/mol]      1.149    0.687\n",
      "    forces mae [kcal/mol/Å]    2.198    2.123\n",
      "epoch:  44                    train:   valid:\n",
      "    loss [a.u.]                5.973    4.039\n",
      "    energy mae [kcal/mol]      1.250    1.007\n",
      "    forces mae [kcal/mol/Å]    2.270    1.892\n",
      "epoch:  45                    train:   valid:\n",
      "    loss [a.u.]                5.579    4.511\n",
      "    energy mae [kcal/mol]      1.332    0.879\n",
      "    forces mae [kcal/mol/Å]    2.140    2.095\n",
      "epoch:  46                    train:   valid:\n",
      "    loss [a.u.]                5.083   10.244\n",
      "    energy mae [kcal/mol]      0.946    1.726\n",
      "    forces mae [kcal/mol/Å]    2.150    3.041\n",
      "epoch:  47                    train:   valid:\n",
      "    loss [a.u.]                6.365    6.056\n",
      "    energy mae [kcal/mol]      1.296    1.505\n",
      "    forces mae [kcal/mol/Å]    2.326    2.212\n",
      "epoch:  48                    train:   valid:\n",
      "    loss [a.u.]                5.355    5.342\n",
      "    energy mae [kcal/mol]      1.210    0.888\n",
      "    forces mae [kcal/mol/Å]    2.152    2.223\n",
      "epoch:  49                    train:   valid:\n",
      "    loss [a.u.]                4.084    3.889\n",
      "    energy mae [kcal/mol]      0.941    0.680\n",
      "    forces mae [kcal/mol/Å]    1.911    1.960\n",
      "epoch:  50                    train:   valid:\n",
      "    loss [a.u.]                4.733    5.641\n",
      "    energy mae [kcal/mol]      1.158    1.036\n",
      "    forces mae [kcal/mol/Å]    1.982    2.205\n",
      "epoch:  51                    train:   valid:\n",
      "    loss [a.u.]                3.838    4.808\n",
      "    energy mae [kcal/mol]      0.780    1.605\n",
      "    forces mae [kcal/mol/Å]    1.908    1.852\n",
      "epoch:  52                    train:   valid:\n",
      "    loss [a.u.]                6.389    5.702\n",
      "    energy mae [kcal/mol]      1.340    1.959\n",
      "    forces mae [kcal/mol/Å]    2.283    1.965\n",
      "epoch:  53                    train:   valid:\n",
      "    loss [a.u.]                4.468    5.680\n",
      "    energy mae [kcal/mol]      0.992    0.848\n",
      "    forces mae [kcal/mol/Å]    2.002    2.350\n",
      "epoch:  54                    train:   valid:\n",
      "    loss [a.u.]                3.822    5.033\n",
      "    energy mae [kcal/mol]      0.723    1.613\n",
      "    forces mae [kcal/mol/Å]    1.907    1.960\n",
      "epoch:  55                    train:   valid:\n",
      "    loss [a.u.]                5.950    3.171\n",
      "    energy mae [kcal/mol]      1.429    0.439\n",
      "    forces mae [kcal/mol/Å]    2.171    1.777\n",
      "epoch:  56                    train:   valid:\n",
      "    loss [a.u.]                3.664    4.808\n",
      "    energy mae [kcal/mol]      0.918    1.338\n",
      "    forces mae [kcal/mol/Å]    1.802    2.072\n",
      "epoch:  57                    train:   valid:\n",
      "    loss [a.u.]                4.534    3.818\n",
      "    energy mae [kcal/mol]      1.200    0.707\n",
      "    forces mae [kcal/mol/Å]    1.953    1.930\n",
      "epoch:  58                    train:   valid:\n",
      "    loss [a.u.]                3.981    3.763\n",
      "    energy mae [kcal/mol]      0.871    0.564\n",
      "    forces mae [kcal/mol/Å]    1.932    1.874\n",
      "epoch:  59                    train:   valid:\n",
      "    loss [a.u.]                3.643    5.927\n",
      "    energy mae [kcal/mol]      0.893    0.899\n",
      "    forces mae [kcal/mol/Å]    1.812    2.455\n",
      "epoch:  60                    train:   valid:\n",
      "    loss [a.u.]                5.687    6.824\n",
      "    energy mae [kcal/mol]      1.324    2.384\n",
      "    forces mae [kcal/mol/Å]    2.170    1.996\n",
      "epoch:  61                    train:   valid:\n",
      "    loss [a.u.]                3.828    5.058\n",
      "    energy mae [kcal/mol]      0.993    1.349\n",
      "    forces mae [kcal/mol/Å]    1.813    1.981\n",
      "epoch:  62                    train:   valid:\n",
      "    loss [a.u.]                3.418    3.124\n",
      "    energy mae [kcal/mol]      0.784    0.473\n",
      "    forces mae [kcal/mol/Å]    1.783    1.724\n",
      "epoch:  63                    train:   valid:\n",
      "    loss [a.u.]                2.886    3.770\n",
      "    energy mae [kcal/mol]      0.692    0.996\n",
      "    forces mae [kcal/mol/Å]    1.663    1.815\n",
      "epoch:  64                    train:   valid:\n",
      "    loss [a.u.]                4.700    3.435\n",
      "    energy mae [kcal/mol]      1.069    0.428\n",
      "    forces mae [kcal/mol/Å]    2.035    1.869\n",
      "epoch:  65                    train:   valid:\n",
      "    loss [a.u.]                3.163    2.714\n",
      "    energy mae [kcal/mol]      0.802    0.483\n",
      "    forces mae [kcal/mol/Å]    1.702    1.676\n",
      "epoch:  66                    train:   valid:\n",
      "    loss [a.u.]                7.851    9.800\n",
      "    energy mae [kcal/mol]      1.682    1.878\n",
      "    forces mae [kcal/mol/Å]    2.388    2.974\n",
      "epoch:  67                    train:   valid:\n",
      "    loss [a.u.]                4.020    3.521\n",
      "    energy mae [kcal/mol]      0.991    0.943\n",
      "    forces mae [kcal/mol/Å]    1.877    1.796\n",
      "epoch:  68                    train:   valid:\n",
      "    loss [a.u.]                3.602    4.190\n",
      "    energy mae [kcal/mol]      0.730    1.472\n",
      "    forces mae [kcal/mol/Å]    1.869    1.758\n",
      "epoch:  69                    train:   valid:\n",
      "    loss [a.u.]                3.943    4.475\n",
      "    energy mae [kcal/mol]      0.873    1.227\n",
      "    forces mae [kcal/mol/Å]    1.891    2.026\n",
      "epoch:  70                    train:   valid:\n",
      "    loss [a.u.]                3.996    2.695\n",
      "    energy mae [kcal/mol]      0.959    0.497\n",
      "    forces mae [kcal/mol/Å]    1.865    1.622\n",
      "epoch:  71                    train:   valid:\n",
      "    loss [a.u.]                3.381    3.302\n",
      "    energy mae [kcal/mol]      0.826    0.492\n",
      "    forces mae [kcal/mol/Å]    1.734    1.838\n",
      "epoch:  72                    train:   valid:\n",
      "    loss [a.u.]                3.359    3.325\n",
      "    energy mae [kcal/mol]      0.849    1.197\n",
      "    forces mae [kcal/mol/Å]    1.745    1.607\n",
      "epoch:  73                    train:   valid:\n",
      "    loss [a.u.]                4.100    3.047\n",
      "    energy mae [kcal/mol]      1.005    0.550\n",
      "    forces mae [kcal/mol/Å]    1.878    1.773\n",
      "epoch:  74                    train:   valid:\n",
      "    loss [a.u.]                3.613    7.589\n",
      "    energy mae [kcal/mol]      0.803    1.872\n",
      "    forces mae [kcal/mol/Å]    1.819    2.509\n",
      "epoch:  75                    train:   valid:\n",
      "    loss [a.u.]                4.175    3.349\n",
      "    energy mae [kcal/mol]      0.952    0.779\n",
      "    forces mae [kcal/mol/Å]    1.931    1.754\n",
      "epoch:  76                    train:   valid:\n",
      "    loss [a.u.]                2.875    3.767\n",
      "    energy mae [kcal/mol]      0.851    0.676\n",
      "    forces mae [kcal/mol/Å]    1.605    2.020\n",
      "epoch:  77                    train:   valid:\n",
      "    loss [a.u.]                3.449    2.638\n",
      "    energy mae [kcal/mol]      0.829    0.844\n",
      "    forces mae [kcal/mol/Å]    1.781    1.502\n",
      "epoch:  78                    train:   valid:\n",
      "    loss [a.u.]                3.444    3.150\n",
      "    energy mae [kcal/mol]      0.957    1.048\n",
      "    forces mae [kcal/mol/Å]    1.706    1.647\n",
      "epoch:  79                    train:   valid:\n",
      "    loss [a.u.]                3.307    2.219\n",
      "    energy mae [kcal/mol]      0.800    0.425\n",
      "    forces mae [kcal/mol/Å]    1.745    1.499\n",
      "epoch:  80                    train:   valid:\n",
      "    loss [a.u.]                3.473    4.032\n",
      "    energy mae [kcal/mol]      0.842    0.609\n",
      "    forces mae [kcal/mol/Å]    1.771    2.030\n",
      "epoch:  81                    train:   valid:\n",
      "    loss [a.u.]                4.825    5.900\n",
      "    energy mae [kcal/mol]      1.005    1.769\n",
      "    forces mae [kcal/mol/Å]    2.075    2.106\n",
      "epoch:  82                    train:   valid:\n",
      "    loss [a.u.]                4.462    3.149\n",
      "    energy mae [kcal/mol]      1.094    0.596\n",
      "    forces mae [kcal/mol/Å]    1.935    1.745\n",
      "epoch:  83                    train:   valid:\n",
      "    loss [a.u.]                3.216    3.302\n",
      "    energy mae [kcal/mol]      0.825    0.663\n",
      "    forces mae [kcal/mol/Å]    1.709    1.791\n",
      "epoch:  84                    train:   valid:\n",
      "    loss [a.u.]                2.890    2.984\n",
      "    energy mae [kcal/mol]      0.869    0.399\n",
      "    forces mae [kcal/mol/Å]    1.584    1.761\n",
      "epoch:  85                    train:   valid:\n",
      "    loss [a.u.]                2.495    3.394\n",
      "    energy mae [kcal/mol]      0.598    0.658\n",
      "    forces mae [kcal/mol/Å]    1.551    1.854\n",
      "epoch:  86                    train:   valid:\n",
      "    loss [a.u.]                3.127    2.143\n",
      "    energy mae [kcal/mol]      0.799    0.365\n",
      "    forces mae [kcal/mol/Å]    1.675    1.467\n",
      "epoch:  87                    train:   valid:\n",
      "    loss [a.u.]                2.718    2.792\n",
      "    energy mae [kcal/mol]      0.665    0.821\n",
      "    forces mae [kcal/mol/Å]    1.600    1.604\n",
      "epoch:  88                    train:   valid:\n",
      "    loss [a.u.]                3.320    3.474\n",
      "    energy mae [kcal/mol]      0.967    1.364\n",
      "    forces mae [kcal/mol/Å]    1.661    1.589\n",
      "epoch:  89                    train:   valid:\n",
      "    loss [a.u.]                3.338    2.476\n",
      "    energy mae [kcal/mol]      0.869    0.432\n",
      "    forces mae [kcal/mol/Å]    1.719    1.566\n",
      "epoch:  90                    train:   valid:\n",
      "    loss [a.u.]                2.988    3.383\n",
      "    energy mae [kcal/mol]      0.920    1.036\n",
      "    forces mae [kcal/mol/Å]    1.615    1.760\n",
      "epoch:  91                    train:   valid:\n",
      "    loss [a.u.]                2.915    4.025\n",
      "    energy mae [kcal/mol]      0.766    0.945\n",
      "    forces mae [kcal/mol/Å]    1.620    1.892\n",
      "epoch:  92                    train:   valid:\n",
      "    loss [a.u.]                3.168    2.600\n",
      "    energy mae [kcal/mol]      0.865    0.463\n",
      "    forces mae [kcal/mol/Å]    1.684    1.654\n",
      "epoch:  93                    train:   valid:\n",
      "    loss [a.u.]                2.294    2.419\n",
      "    energy mae [kcal/mol]      0.575    0.912\n",
      "    forces mae [kcal/mol/Å]    1.485    1.421\n",
      "epoch:  94                    train:   valid:\n",
      "    loss [a.u.]                2.381    5.130\n",
      "    energy mae [kcal/mol]      0.705    1.453\n",
      "    forces mae [kcal/mol/Å]    1.485    2.034\n",
      "epoch:  95                    train:   valid:\n",
      "    loss [a.u.]                4.997    4.238\n",
      "    energy mae [kcal/mol]      1.195    1.267\n",
      "    forces mae [kcal/mol/Å]    2.058    1.866\n",
      "epoch:  96                    train:   valid:\n",
      "    loss [a.u.]                2.900    2.924\n",
      "    energy mae [kcal/mol]      0.804    0.839\n",
      "    forces mae [kcal/mol/Å]    1.628    1.615\n",
      "epoch:  97                    train:   valid:\n",
      "    loss [a.u.]                2.327    2.689\n",
      "    energy mae [kcal/mol]      0.619    0.847\n",
      "    forces mae [kcal/mol/Å]    1.488    1.543\n",
      "epoch:  98                    train:   valid:\n",
      "    loss [a.u.]                3.039    3.684\n",
      "    energy mae [kcal/mol]      0.923    0.498\n",
      "    forces mae [kcal/mol/Å]    1.608    1.925\n",
      "epoch:  99                    train:   valid:\n",
      "    loss [a.u.]                2.952    3.039\n",
      "    energy mae [kcal/mol]      0.725    0.599\n",
      "    forces mae [kcal/mol/Å]    1.655    1.779\n",
      "epoch:  100                    train:   valid:\n",
      "    loss [a.u.]                3.519    3.615\n",
      "    energy mae [kcal/mol]      0.967    1.039\n",
      "    forces mae [kcal/mol/Å]    1.761    1.747\n"
     ]
    }
   ],
   "source": [
    "# Create PRNGKeys.\n",
    "data_key, train_key = jax.random.split(jax.random.PRNGKey(0), 2)\n",
    "\n",
    "# Draw training and validation sets.\n",
    "train_data, valid_data, _ = prepare_datasets(data_key, num_train=num_train, num_valid=num_valid)\n",
    "\n",
    "# Create and train model.\n",
    "message_passing_model = MessagePassingModel(\n",
    "  features=features,\n",
    "  max_degree=max_degree,\n",
    "  num_iterations=num_iterations,\n",
    "  num_basis_functions=num_basis_functions,\n",
    "  cutoff=cutoff,\n",
    ")\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=message_passing_model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  forces_weight=forces_weight,\n",
    "  batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import flax  # Ensure flax is imported for serialization\n",
    "# serialized_params = flax.serialization.to_bytes(params)\n",
    "# with open('model_params.bin', 'wb') as f:\n",
    "#     f.write(serialized_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized model parameters saved to: Model\\model_params.bin\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"Model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "serialized_params = flax.serialization.to_bytes(params)\n",
    "params_file_path = os.path.join(model_dir, \"model_params.bin\")\n",
    "with open(params_file_path, \"wb\") as f:\n",
    "    f.write(serialized_params)\n",
    "print(f\"Serialized model parameters saved to: {params_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to: Model\\model_checkpoint.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model as a .pkl file\n",
    "import pickle\n",
    "model_checkpoint = {\n",
    "    \"model\": message_passing_model,  # Save the model architecture\n",
    "    \"params\": params,  # Save the trained model parameters\n",
    "    \"features\": features,\n",
    "    \"max_degree\": max_degree,\n",
    "    \"num_iterations\": num_iterations,\n",
    "    \"num_basis_functions\": num_basis_functions,\n",
    "    \"cutoff\": cutoff\n",
    "}\n",
    "checkpoint_file_path = os.path.join(model_dir, \"model_checkpoint.pkl\")\n",
    "with open(checkpoint_file_path, \"wb\") as f:\n",
    "    pickle.dump(model_checkpoint, f)\n",
    "print(f\"Model checkpoint saved to: {checkpoint_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_theta(G_list, F_list, beta):\n",
    "    \"\"\"\n",
    "    Compute Fisher-optimal parameters\n",
    "    \n",
    "    Args:\n",
    "        G_list: List of gradient matrices\n",
    "        F_list: List of force matrices\n",
    "        beta: Temperature factor\n",
    "    \"\"\"\n",
    "    GGT = [jnp.dot(G.T, G) for G in G_list]\n",
    "    \n",
    "    c_list = jnp.array( [(beta**2) * jnp.dot(G_list[i].T, F_list[i]) for i in range(len(G_list))])\n",
    "    print(\"ok\")\n",
    "    # Average over samples\n",
    "    \n",
    "    stacked_c = jnp.mean(jnp.stack(c_list), axis=0)\n",
    "    stacked_GGT = jnp.mean(jnp.stack(GGT), axis=0)\n",
    "    \n",
    "    # Compute temperature-scaled matrix\n",
    "    T = beta**2 * stacked_GGT\n",
    "    \n",
    "    # Solve linear system\n",
    "    theta_dot = jnp.linalg.solve(T + 1e-6 * jnp.eye(T.shape[0]), stacked_c)\n",
    "    #print(theta_dot)\n",
    "    return theta_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fisher_model(nn.Module):  # Doesn't inherit from MessagePassingModel\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118\n",
    "\n",
    "    @nn.compact  # Important for parameter initialization\n",
    "    def __call__(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1, features=self.features)(atomic_numbers)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                y = e3x.nn.MessagePass(max_degree=0, include_pseudotensors=False)(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                x = e3x.nn.change_max_degree_or_type(x, max_degree=0, include_pseudotensors=False)\n",
    "            else:\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "\n",
    "        # 5. Predict atomic energies with an ordinary dense layer.\n",
    "        element_bias = self.param('element_bias', lambda rng, shape: jnp.zeros(shape), (self.max_atomic_number + 1))\n",
    "        atomic_energies = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)  # (..., Natoms, 1, 1, 1)\n",
    "        atomic_energies = jnp.squeeze(atomic_energies, axis=(-1, -2, -3))  # Squeeze last 3 dimensions.\n",
    "        atomic_energies += element_bias[atomic_numbers]\n",
    "\n",
    "        # 6. Sum atomic energies to obtain the total energy.\n",
    "        energy = jax.ops.segment_sum(atomic_energies, segment_ids=batch_segments, num_segments=batch_size)\n",
    "\n",
    "        return -jnp.sum(energy), energy  # Return energy and forces (negative gradient)\n",
    "    \n",
    "    @nn.compact\n",
    "    def energy_only(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size):\n",
    "        energy, _ = self(atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size)\n",
    "        return energy\n",
    "    \n",
    "    @nn.compact\n",
    "    def get_grad_desc(self, params, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "        \n",
    "        print(displacements.shape)\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "        )\n",
    "        \n",
    "        print(basis.shape)\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1, features=self.features)(atomic_numbers)\n",
    "        print(x.shape)\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                y = e3x.nn.MessagePass(max_degree=0, include_pseudotensors=False)(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                x = e3x.nn.change_max_degree_or_type(x, max_degree=0, include_pseudotensors=False)\n",
    "            else:\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "\n",
    "        # 5. Get the activations before the last layer\n",
    "        x = jax.lax.stop_gradient(x)  # Freeze all the layers before this one\n",
    "        print(x.shape)\n",
    "        # 6. Calculate the gradient of the output with respect to x\n",
    "        @nn.compact\n",
    "        def energy_no_grad(x_local):\n",
    "            element_bias = self.param('element_bias', lambda rng, shape: jnp.zeros(shape), (self.max_atomic_number+1))\n",
    "            #atomic_energies = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)  # (..., Natoms, 1, 1, 1)\n",
    "            atomic_energies = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x_local)  # (..., Natoms, 1, 1, 1)\n",
    "            atomic_energies = jnp.squeeze(atomic_energies, axis=(-1, -2, -3))  # Squeeze last 3 dimensions.\n",
    "            atomic_energies += element_bias[atomic_numbers]\n",
    "            energy = jax.ops.segment_sum(atomic_energies, segment_ids=batch_segments, num_segments=batch_size)\n",
    "            return jnp.sum(energy)\n",
    "\n",
    "        grad_desc = jax.grad(energy_no_grad)(x)\n",
    "        return grad_desc\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def get_layer_gradients(self, params, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # # 2. Expand displacement vectors in basis functions.\n",
    "        # basis = e3x.nn.basis(\n",
    "        #     displacements,\n",
    "        #     num=self.num_basis_functions,\n",
    "        #     max_degree=self.max_degree,\n",
    "        #     radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "        #     cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "        # )\n",
    "\n",
    "        @nn.compact\n",
    "        # Final layer gradient computation\n",
    "        def displacement_grad(local_displacements):\n",
    "            # # Recreate the entire network computation with input displacements\n",
    "            local_basis = e3x.nn.basis(\n",
    "                local_displacements,\n",
    "                num=self.num_basis_functions,\n",
    "                max_degree=self.max_degree,\n",
    "                radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "                cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "            )\n",
    "\n",
    "            # Embed atomic numbers\n",
    "            local_x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1, features=self.features)(atomic_numbers)\n",
    "\n",
    "            # Perform message passing iterations\n",
    "            for i in range(self.num_iterations):\n",
    "                if i == self.num_iterations - 1:\n",
    "                    local_y = e3x.nn.MessagePass(max_degree=0, include_pseudotensors=False)(local_x, local_basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                    local_x = e3x.nn.change_max_degree_or_type(local_x, max_degree=0, include_pseudotensors=False)\n",
    "                else:\n",
    "                    local_y = e3x.nn.MessagePass()(local_x, local_basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                local_y = e3x.nn.add(local_x, local_y)\n",
    "\n",
    "                # Atom-wise refinement MLP\n",
    "                local_y = e3x.nn.Dense(self.features)(local_y)\n",
    "                local_y = e3x.nn.silu(local_y)\n",
    "                local_y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(local_y)\n",
    "\n",
    "                # Residual connection\n",
    "                local_x = e3x.nn.add(local_x, local_y)\n",
    "            \n",
    "            return local_x\n",
    "            # # Final layer energy computation\n",
    "            # element_bias = self.param('element_bias', lambda rng, shape: jnp.zeros(shape), (self.max_atomic_number+1))\n",
    "            # dense_layer = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)\n",
    "            # atomic_energies = dense_layer(local_x)\n",
    "\n",
    "            #return atomic_energies\n",
    "            # atomic_energies = jnp.squeeze(atomic_energies, axis=(-1, -2, -3))\n",
    "            # atomic_energies += element_bias[atomic_numbers]\n",
    "            \n",
    "            # energy = jax.ops.segment_sum(atomic_energies, segment_ids=batch_segments, num_segments=batch_size)\n",
    "            # return jnp.sum(energy)\n",
    "\n",
    "        # Compute gradients with respect to displacements\n",
    "        layer_gradients = jax.jacobian(displacement_grad)(displacements)\n",
    "        \n",
    "        return layer_gradients\n",
    "\n",
    "    \n",
    "    @nn.compact\n",
    "    def apply_fisher(self, params, batch, beta, batch_size):\n",
    "        G = self.get_layer_gradients(params, batch['atomic_numbers'], batch[\"positions\"], batch['dst_idx'], batch['src_idx'],\n",
    "                               batch['batch_segments'], batch_size)\n",
    "        \n",
    "        G = jnp.mean(G, axis=4)\n",
    "        G = jnp.squeeze(G, axis=(1, 2))\n",
    "        G = jnp.transpose(G, (0, 2, 1))\n",
    "        print(G.shape)\n",
    "        G = G.astype(jnp.float16) \n",
    "        F = batch['forces']  # Target forces\n",
    "\n",
    "        print(\"G shape\", G.shape)\n",
    "        print(\"F shape\", F.shape)\n",
    "\n",
    "        theta_fisher = fisher_theta(G, F, beta)  # Assuming you have this function defined\n",
    "        return theta_fisher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 3, 32)\n",
      "G shape (90, 3, 32)\n",
      "F shape (90, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexa\\OneDrive\\Bureau\\MD\\MD_tdlog\\.venv\\Lib\\site-packages\\jax\\_src\\interpreters\\xla.py:135: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: Buffer Definition Event: Error dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError preparing computation: %sOut of memory allocating 8493465600 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#     theta_fisher = fisher_model_instance.apply_fisher(new_params, batch, to_beta, batch_size)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m theta_fisher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtheta_fisher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m new_params\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m     60\u001b[0m     new_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_5\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m theta_fisher\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# Update the last layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alexa\\OneDrive\\Bureau\\MD\\MD_tdlog\\.venv\\Lib\\site-packages\\jax\\_src\\array.py:283\u001b[0m, in \u001b[0;36mArrayImpl.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Alexa\\OneDrive\\Bureau\\MD\\MD_tdlog\\.venv\\Lib\\site-packages\\jax\\_src\\profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[1;32mc:\\Users\\Alexa\\OneDrive\\Bureau\\MD\\MD_tdlog\\.venv\\Lib\\site-packages\\jax\\_src\\array.py:627\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[1;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: Buffer Definition Event: Error dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError dispatching computation: %sError preparing computation: %sOut of memory allocating 8493465600 bytes."
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "new_params = deepcopy(params)\n",
    "# Create fisher_model instance\n",
    "fisher_model_instance = fisher_model(\n",
    "    features=features,\n",
    "    max_degree=max_degree,\n",
    "    num_iterations=num_iterations,\n",
    "    num_basis_functions=num_basis_functions,\n",
    "    cutoff=cutoff,\n",
    ")\n",
    "\n",
    "# *** Initialize the fisher_model_instance ***\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "dummy_data = train_batches[0]  # Get a batch for initialization\n",
    "num_atoms = len(valid_data['atomic_numbers'])\n",
    "batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "new_params = fisher_model_instance.init(\n",
    "    key,\n",
    "    atomic_numbers=dummy_data['atomic_numbers'],\n",
    "    positions=dummy_data['positions'],\n",
    "    dst_idx=dummy_data['dst_idx'],\n",
    "    src_idx=dummy_data['src_idx'],\n",
    "    batch_segments=batch_segments, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Convert new_params to a FrozenDict\n",
    "new_params = flax.core.freeze(new_params)\n",
    "new_params = new_params.unfreeze()\n",
    "# print(new_params)\n",
    "# Update parameters (Corrected)\n",
    "for k, v in params.items():\n",
    "    if 'Dense_5' not in k:  # Skip the last layer\n",
    "        new_params['params'][k] = v  # Access the correct level 'param'\n",
    "\n",
    "# Freeze new_params again\n",
    "new_params = flax.core.freeze(new_params)\n",
    "from scipy.constants import Boltzmann\n",
    "temperature = 300\n",
    "to_beta = 1 / (Boltzmann * temperature)\n",
    "\n",
    "# Apply Fisher update and update the last layer's weights for ONE batch\n",
    "batch = next(iter(train_batches)) # Get one batch\n",
    "theta_fisher = fisher_model_instance.apply(\n",
    "    params,\n",
    "    method=fisher_model_instance.apply_fisher,\n",
    "    params=new_params,\n",
    "    batch=batch,\n",
    "    beta=to_beta,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "#     theta_fisher = fisher_model_instance.apply_fisher(new_params, batch, to_beta, batch_size)\n",
    "if theta_fisher is not None:\n",
    "    print(theta_fisher)\n",
    "    new_params = new_params.unfreeze()\n",
    "    new_params['params']['Dense_5']['kernel'] = theta_fisher.T  # Update the last layer\n",
    "    new_params = flax.core.freeze(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to: Model\\fisher_model.pkl\n",
      "Serialized model parameters saved to: Model\\fisher_model_params.bin\n"
     ]
    }
   ],
   "source": [
    "fisher_model_checkpoint = {\n",
    "    \"model\": message_passing_model,  # Save the model architecture\n",
    "    \"params\": new_params,  # Save the trained model parameters\n",
    "    \"features\": features,\n",
    "    \"max_degree\": max_degree,\n",
    "    \"num_iterations\": num_iterations,\n",
    "    \"num_basis_functions\": num_basis_functions,\n",
    "    \"cutoff\": cutoff\n",
    "}\n",
    "checkpoint_file_path = os.path.join(model_dir, \"fisher_model.pkl\")\n",
    "with open(checkpoint_file_path, \"wb\") as f:\n",
    "    pickle.dump(fisher_model_checkpoint, f)\n",
    "print(f\"Model checkpoint saved to: {checkpoint_file_path}\")\n",
    "\n",
    "\n",
    "serialized_fisher_params = flax.serialization.to_bytes(new_params)\n",
    "fparams_file_path = os.path.join(model_dir, \"fisher_model_params.bin\")\n",
    "with open(fparams_file_path, \"wb\") as f:\n",
    "    f.write(serialized_fisher_params)\n",
    "print(f\"Serialized model parameters saved to: {fparams_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'flax.core.frozen_dict.FrozenDict'>\n"
     ]
    }
   ],
   "source": [
    "# checking difference between params and new_params\n",
    "print(type(params))\n",
    "print(type(new_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core.frozen_dict import FrozenDict\n",
    "import jax.numpy as jnp\n",
    "from jaxlib.xla_extension import ArrayImpl\n",
    "\n",
    "def unfreeze_if_needed(d):\n",
    "    \"\"\"Recursively unfreezes a Flax FrozenDict if necessary.\"\"\"\n",
    "    if isinstance(d, FrozenDict):\n",
    "        return {k: unfreeze_if_needed(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "def compare_dict_structure(dict1, dict2, path=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively compares the structure of two dictionaries, handling Flax FrozenDicts and JAX ArrayImpl.\n",
    "\n",
    "    :param dict1: First dictionary (or FrozenDict).\n",
    "    :param dict2: Second dictionary (or FrozenDict).\n",
    "    :param path: Current path in the dictionary (for debugging).\n",
    "    :return: List of differences found.\n",
    "    \"\"\"\n",
    "    # Unfreeze both dictionaries if they are FrozenDicts\n",
    "    dict1 = unfreeze_if_needed(dict1)\n",
    "    dict2 = unfreeze_if_needed(dict2)\n",
    "\n",
    "    differences = []\n",
    "\n",
    "    # If one is an ArrayImpl and the other is not, report a mismatch\n",
    "    if isinstance(dict1, ArrayImpl) and isinstance(dict2, ArrayImpl):\n",
    "        return differences  # Structure is fine, do not compare actual values\n",
    "\n",
    "    if isinstance(dict1, ArrayImpl) or isinstance(dict2, ArrayImpl):\n",
    "        differences.append(f\"Mismatch at {path}: One is ArrayImpl, the other is not.\")\n",
    "        return differences\n",
    "    \n",
    "    # Check if both are dictionaries\n",
    "    if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
    "        differences.append(f\"Mismatch at {path}: One is not a dictionary.\")\n",
    "        return differences\n",
    "    \n",
    "    # Get key sets\n",
    "    keys1 = set(dict1.keys())\n",
    "    keys2 = set(dict2.keys())\n",
    "    \n",
    "    missing_in_dict2 = keys1 - keys2\n",
    "    missing_in_dict1 = keys2 - keys1\n",
    "    \n",
    "    if missing_in_dict2:\n",
    "        differences.append(f\"Keys missing in second dictionary at {path}: {missing_in_dict2}\")\n",
    "    if missing_in_dict1:\n",
    "        differences.append(f\"Keys missing in first dictionary at {path}: {missing_in_dict1}\")\n",
    "    \n",
    "    # Recursively check sub-dictionaries\n",
    "    for key in keys1 & keys2:\n",
    "        new_path = f\"{path}.{key}\" if path else key\n",
    "        differences.extend(compare_dict_structure(dict1[key], dict2[key], new_path))\n",
    "    \n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core.frozen_dict import FrozenDict\n",
    "import jax.numpy as jnp\n",
    "from jaxlib.xla_extension import ArrayImpl\n",
    "\n",
    "def unfreeze_if_needed(d):\n",
    "    \"\"\"Recursively unfreezes a Flax FrozenDict if necessary.\"\"\"\n",
    "    if isinstance(d, FrozenDict):\n",
    "        return {k: unfreeze_if_needed(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "def compare_dict_structure_and_values(dict1, dict2, path=\"\", tol=1e-5):\n",
    "    \"\"\"\n",
    "    Recursively compares the structure of two dictionaries and values inside JAX ArrayImpl objects.\n",
    "\n",
    "    :param dict1: First dictionary (or FrozenDict).\n",
    "    :param dict2: Second dictionary (or FrozenDict).\n",
    "    :param path: Current path in the dictionary (for debugging).\n",
    "    :param tol: Tolerance for comparing floating-point values in arrays.\n",
    "    :return: List of differences found.\n",
    "    \"\"\"\n",
    "    # Unfreeze both dictionaries if they are FrozenDicts\n",
    "    dict1 = unfreeze_if_needed(dict1)\n",
    "    dict2 = unfreeze_if_needed(dict2)\n",
    "\n",
    "    differences = []\n",
    "\n",
    "    # If both are JAX arrays, compare their values\n",
    "    if isinstance(dict1, ArrayImpl) and isinstance(dict2, ArrayImpl):\n",
    "        if not jnp.allclose(dict1, dict2, atol=tol, rtol=tol):\n",
    "            differences.append(f\"Value mismatch at {path}: Arrays differ beyond tolerance {tol}.\")\n",
    "        return differences  # No structural differences at this level\n",
    "\n",
    "    # If one is an ArrayImpl and the other is not, report a type mismatch\n",
    "    if isinstance(dict1, ArrayImpl) or isinstance(dict2, ArrayImpl):\n",
    "        differences.append(f\"Type mismatch at {path}: One is ArrayImpl, the other is not.\")\n",
    "        return differences\n",
    "    \n",
    "    # Ensure both are dictionaries\n",
    "    if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
    "        differences.append(f\"Type mismatch at {path}: One is not a dictionary.\")\n",
    "        return differences\n",
    "\n",
    "    # Get key sets\n",
    "    keys1 = set(dict1.keys())\n",
    "    keys2 = set(dict2.keys())\n",
    "    \n",
    "    missing_in_dict2 = keys1 - keys2\n",
    "    missing_in_dict1 = keys2 - keys1\n",
    "    \n",
    "    if missing_in_dict2:\n",
    "        differences.append(f\"Keys missing in second dictionary at {path}: {missing_in_dict2}\")\n",
    "    if missing_in_dict1:\n",
    "        differences.append(f\"Keys missing in first dictionary at {path}: {missing_in_dict1}\")\n",
    "    \n",
    "    # Recursively check sub-dictionaries\n",
    "    for key in keys1 & keys2:\n",
    "        new_path = f\"{path}.{key}\" if path else key\n",
    "        differences.extend(compare_dict_structure_and_values(dict1[key], dict2[key], new_path, tol))\n",
    "    \n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_structure(d, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the structure of a dictionary.\n",
    "    \n",
    "    :param d: Dictionary to print structure.\n",
    "    :param indent: Current indentation level (used for recursion).\n",
    "    \"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        print(\" \" * indent + f\"- (Value: {type(d).__name__})\")  # Print value type if not a dict\n",
    "        return\n",
    "    \n",
    "    for key, value in d.items():\n",
    "        print(\" \" * indent + f\"- {key}: {type(value).__name__}\")  # Print key and value type\n",
    "        if isinstance(value, dict):\n",
    "            print_dict_structure(value, indent + 4)  # Recursive call with increased indentationd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- params: dict\n",
      "    - Dense_0: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_1: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_2: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 0-: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1+: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_3: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 0-: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1+: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_4: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_5: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_6: dict\n",
      "        - kernel: ArrayImpl\n",
      "    - Embed_0: dict\n",
      "        - embedding: ArrayImpl\n",
      "    - MessagePass_0: dict\n",
      "        - filter: dict\n",
      "            - 0+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - tensor: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - MessagePass_1: dict\n",
      "        - filter: dict\n",
      "            - 0+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - tensor: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - MessagePass_2: dict\n",
      "        - filter: dict\n",
      "            - 0+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - tensor: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - element_bias: ArrayImpl\n"
     ]
    }
   ],
   "source": [
    "print_dict_structure(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- params: dict\n",
      "    - Dense_0: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_1: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_2: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 0-: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1+: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_3: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - 0-: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1+: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - 1-: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_4: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "    - Dense_5: dict\n",
      "        - 0+: dict\n",
      "            - bias: ArrayImpl\n",
      "            - kernel: ArrayImpl\n",
      "        - kernel: ArrayImpl\n",
      "    - Dense_6: dict\n",
      "        - kernel: ArrayImpl\n",
      "    - Embed_0: dict\n",
      "        - embedding: ArrayImpl\n",
      "    - MessagePass_0: dict\n",
      "        - filter: dict\n",
      "            - 0+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - tensor: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - MessagePass_1: dict\n",
      "        - filter: dict\n",
      "            - 0+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - tensor: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - MessagePass_2: dict\n",
      "        - filter: dict\n",
      "            - 0+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - tensor: dict\n",
      "            - kernel: ArrayImpl\n",
      "    - element_bias: ArrayImpl\n",
      "    - params: dict\n",
      "        - Dense_0: dict\n",
      "            - 0+: dict\n",
      "                - bias: ArrayImpl\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - Dense_1: dict\n",
      "            - 0+: dict\n",
      "                - bias: ArrayImpl\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - Dense_2: dict\n",
      "            - 0+: dict\n",
      "                - bias: ArrayImpl\n",
      "                - kernel: ArrayImpl\n",
      "            - 0-: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - Dense_3: dict\n",
      "            - 0+: dict\n",
      "                - bias: ArrayImpl\n",
      "                - kernel: ArrayImpl\n",
      "            - 0-: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1+: dict\n",
      "                - kernel: ArrayImpl\n",
      "            - 1-: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - Dense_4: dict\n",
      "            - 0+: dict\n",
      "                - bias: ArrayImpl\n",
      "                - kernel: ArrayImpl\n",
      "        - Dense_5: dict\n",
      "            - 0+: dict\n",
      "                - bias: ArrayImpl\n",
      "                - kernel: ArrayImpl\n",
      "        - Dense_6: dict\n",
      "            - kernel: ArrayImpl\n",
      "        - Embed_0: dict\n",
      "            - embedding: ArrayImpl\n",
      "        - MessagePass_0: dict\n",
      "            - filter: dict\n",
      "                - 0+: dict\n",
      "                    - kernel: ArrayImpl\n",
      "                - 1-: dict\n",
      "                    - kernel: ArrayImpl\n",
      "            - tensor: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - MessagePass_1: dict\n",
      "            - filter: dict\n",
      "                - 0+: dict\n",
      "                    - kernel: ArrayImpl\n",
      "                - 1-: dict\n",
      "                    - kernel: ArrayImpl\n",
      "            - tensor: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - MessagePass_2: dict\n",
      "            - filter: dict\n",
      "                - 0+: dict\n",
      "                    - kernel: ArrayImpl\n",
      "                - 1-: dict\n",
      "                    - kernel: ArrayImpl\n",
      "            - tensor: dict\n",
      "                - kernel: ArrayImpl\n",
      "        - element_bias: ArrayImpl\n"
     ]
    }
   ],
   "source": [
    "print_dict_structure(new_params.unfreeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Keys missing in first dictionary at params: {'params'}\",\n",
       " \"Keys missing in first dictionary at params.Dense_5: {'kernel'}\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_dict_structure(params, new_params.unfreeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Keys missing in first dictionary at params: {'params'}\",\n",
       " 'Value mismatch at params.Dense_3.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_3.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_3.0+.bias: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_2.filter.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_2.filter.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_2.tensor.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_4.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_4.0+.bias: Arrays differ beyond tolerance 1e-05.',\n",
       " \"Keys missing in first dictionary at params.Dense_5: {'kernel'}\",\n",
       " 'Value mismatch at params.Dense_5.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_5.0+.bias: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_6.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.element_bias: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_0.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_0.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_0.0+.bias: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_2.1+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_2.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_2.0-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_2.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_2.0+.bias: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_1.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_1.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Dense_1.0+.bias: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.Embed_0.embedding: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_0.filter.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_0.filter.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_0.tensor.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_1.filter.1-.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_1.filter.0+.kernel: Arrays differ beyond tolerance 1e-05.',\n",
       " 'Value mismatch at params.MessagePass_1.tensor.kernel: Arrays differ beyond tolerance 1e-05.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_dict_structure_and_values(params, new_params.unfreeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
